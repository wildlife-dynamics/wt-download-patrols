# AUTOGENERATED BY ECOSCOPE-WORKFLOWS; see fingerprint in README.md for details
import json
import os

from ecoscope_workflows_core.tasks.config import set_string_var as set_string_var
from ecoscope_workflows_core.tasks.config import (
    set_workflow_details as set_workflow_details,
)
from ecoscope_workflows_core.tasks.filter import set_time_range as set_time_range
from ecoscope_workflows_core.tasks.groupby import groupbykey as groupbykey
from ecoscope_workflows_core.tasks.groupby import set_groupers as set_groupers
from ecoscope_workflows_core.tasks.groupby import split_groups as split_groups
from ecoscope_workflows_core.tasks.io import persist_text as persist_text
from ecoscope_workflows_core.tasks.io import set_er_connection as set_er_connection
from ecoscope_workflows_core.tasks.results import (
    create_map_widget_single_view as create_map_widget_single_view,
)
from ecoscope_workflows_core.tasks.results import gather_dashboard as gather_dashboard
from ecoscope_workflows_core.tasks.results import (
    merge_widget_views as merge_widget_views,
)
from ecoscope_workflows_core.tasks.skip import (
    all_keyed_iterables_are_skips as all_keyed_iterables_are_skips,
)
from ecoscope_workflows_core.tasks.skip import (
    any_dependency_skipped as any_dependency_skipped,
)
from ecoscope_workflows_core.tasks.skip import any_is_empty_df as any_is_empty_df
from ecoscope_workflows_core.tasks.transformation import (
    add_temporal_index as add_temporal_index,
)
from ecoscope_workflows_core.tasks.transformation import (
    convert_column_values_to_string as convert_column_values_to_string,
)
from ecoscope_workflows_core.tasks.transformation import map_columns as map_columns
from ecoscope_workflows_ext_ecoscope.tasks.io import (
    get_patrol_events as get_patrol_events,
)
from ecoscope_workflows_ext_ecoscope.tasks.io import (
    get_patrol_observations as get_patrol_observations,
)
from ecoscope_workflows_ext_ecoscope.tasks.io import persist_df as persist_df
from ecoscope_workflows_ext_ecoscope.tasks.io import (
    set_patrol_status as set_patrol_status,
)
from ecoscope_workflows_ext_ecoscope.tasks.io import (
    set_patrol_types as set_patrol_types,
)
from ecoscope_workflows_ext_ecoscope.tasks.preprocessing import (
    process_relocations as process_relocations,
)
from ecoscope_workflows_ext_ecoscope.tasks.preprocessing import (
    relocations_to_trajectory as relocations_to_trajectory,
)
from ecoscope_workflows_ext_ecoscope.tasks.results import (
    create_point_layer as create_point_layer,
)
from ecoscope_workflows_ext_ecoscope.tasks.results import (
    create_polyline_layer as create_polyline_layer,
)
from ecoscope_workflows_ext_ecoscope.tasks.results import draw_ecomap as draw_ecomap
from ecoscope_workflows_ext_ecoscope.tasks.results import set_base_maps as set_base_maps
from ecoscope_workflows_ext_ecoscope.tasks.transformation import (
    apply_color_map as apply_color_map,
)
from ecoscope_workflows_ext_ecoscope.tasks.transformation import (
    apply_reloc_coord_filter as apply_reloc_coord_filter,
)

from ..params import Params


def main(params: Params):
    params_dict = json.loads(params.model_dump_json(exclude_unset=True))

    workflow_details = (
        set_workflow_details.validate()
        .set_task_instance_id("workflow_details")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(**(params_dict.get("workflow_details") or {}))
        .call()
    )

    er_client_name = (
        set_er_connection.validate()
        .set_task_instance_id("er_client_name")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(**(params_dict.get("er_client_name") or {}))
        .call()
    )

    time_range = (
        set_time_range.validate()
        .set_task_instance_id("time_range")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            time_format="%d %b %Y %H:%M:%S %Z", **(params_dict.get("time_range") or {})
        )
        .call()
    )

    er_patrol_types = (
        set_patrol_types.validate()
        .set_task_instance_id("er_patrol_types")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(**(params_dict.get("er_patrol_types") or {}))
        .call()
    )

    er_patrol_status = (
        set_patrol_status.validate()
        .set_task_instance_id("er_patrol_status")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(**(params_dict.get("er_patrol_status") or {}))
        .call()
    )

    patrol_obs = (
        get_patrol_observations.validate()
        .set_task_instance_id("patrol_obs")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            client=er_client_name,
            time_range=time_range,
            patrol_types=er_patrol_types,
            status=er_patrol_status,
            include_patrol_details=True,
            raise_on_empty=False,
            **(params_dict.get("patrol_obs") or {}),
        )
        .call()
    )

    fetch_patrol_events = (
        get_patrol_events.validate()
        .set_task_instance_id("fetch_patrol_events")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            client=er_client_name,
            time_range=time_range,
            patrol_types=er_patrol_types,
            status=er_patrol_status,
            truncate_to_time_range=True,
            raise_on_empty=False,
            **(params_dict.get("fetch_patrol_events") or {}),
        )
        .call()
    )

    groupers = (
        set_groupers.validate()
        .set_task_instance_id("groupers")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(**(params_dict.get("groupers") or {}))
        .call()
    )

    patrol_reloc = (
        process_relocations.validate()
        .set_task_instance_id("patrol_reloc")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            observations=patrol_obs,
            relocs_columns=[
                "patrol_id",
                "patrol_start_time",
                "patrol_end_time",
                "patrol_type__value",
                "patrol_type__display",
                "patrol_serial_number",
                "patrol_status",
                "patrol_subject",
                "groupby_col",
                "fixtime",
                "junk_status",
                "extra__source",
                "geometry",
            ],
            filter_point_coords=[
                {"x": 180.0, "y": 90.0},
                {"x": 0.0, "y": 0.0},
                {"x": 1.0, "y": 1.0},
            ],
            **(params_dict.get("patrol_reloc") or {}),
        )
        .call()
    )

    set_patrol_traj_color_column = (
        set_string_var.validate()
        .set_task_instance_id("set_patrol_traj_color_column")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            var="patrol_type", **(params_dict.get("set_patrol_traj_color_column") or {})
        )
        .call()
    )

    patrol_traj = (
        relocations_to_trajectory.validate()
        .set_task_instance_id("patrol_traj")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(relocations=patrol_reloc, **(params_dict.get("patrol_traj") or {}))
        .call()
    )

    traj_add_temporal_index = (
        add_temporal_index.validate()
        .set_task_instance_id("traj_add_temporal_index")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            df=patrol_traj,
            time_col="extra__patrol_start_time",
            groupers=groupers,
            cast_to_datetime=True,
            format="mixed",
            **(params_dict.get("traj_add_temporal_index") or {}),
        )
        .call()
    )

    traj_rename_grouper_columns = (
        map_columns.validate()
        .set_task_instance_id("traj_rename_grouper_columns")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            df=traj_add_temporal_index,
            drop_columns=[],
            retain_columns=[],
            rename_columns={
                "extra__patrol_type__value": "patrol_type",
                "extra__patrol_serial_number": "patrol_serial_number",
                "extra__patrol_status": "patrol_status",
                "extra__patrol_subject": "patrol_subject",
            },
            **(params_dict.get("traj_rename_grouper_columns") or {}),
        )
        .call()
    )

    traj_colormap = (
        apply_color_map.validate()
        .set_task_instance_id("traj_colormap")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            df=traj_rename_grouper_columns,
            colormap=[
                "#FF9600",
                "#F23B0E",
                "#A100CB",
                "#F04564",
                "#03421A",
                "#3089FF",
                "#E26FFF",
                "#8C1700",
                "#002960",
                "#FFD000",
                "#B62879",
                "#680078",
                "#005A56",
                "#0056C7",
                "#331878",
                "#E76826",
            ],
            input_column_name=set_patrol_traj_color_column,
            output_column_name="patrol_traj_colormap",
            **(params_dict.get("traj_colormap") or {}),
        )
        .call()
    )

    filter_fetched_patrol_events = (
        apply_reloc_coord_filter.validate()
        .set_task_instance_id("filter_fetched_patrol_events")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            df=fetch_patrol_events,
            roi_gdf=None,
            roi_name=None,
            **(params_dict.get("filter_fetched_patrol_events") or {}),
        )
        .call()
    )

    pe_add_temporal_index = (
        add_temporal_index.validate()
        .set_task_instance_id("pe_add_temporal_index")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            df=filter_fetched_patrol_events,
            time_col="patrol_start_time",
            groupers=groupers,
            cast_to_datetime=True,
            format="mixed",
            **(params_dict.get("pe_add_temporal_index") or {}),
        )
        .call()
    )

    pe_colormap = (
        apply_color_map.validate()
        .set_task_instance_id("pe_colormap")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            df=pe_add_temporal_index,
            input_column_name="event_type",
            colormap="tab20b",
            output_column_name="event_type_colormap",
            **(params_dict.get("pe_colormap") or {}),
        )
        .call()
    )

    patrol_traj_cols_to_string = (
        convert_column_values_to_string.validate()
        .set_task_instance_id("patrol_traj_cols_to_string")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            df=traj_colormap,
            columns=["patrol_serial_number", "patrol_type"],
            **(params_dict.get("patrol_traj_cols_to_string") or {}),
        )
        .call()
    )

    pe_cols_to_string = (
        convert_column_values_to_string.validate()
        .set_task_instance_id("pe_cols_to_string")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            df=pe_colormap,
            columns=["patrol_serial_number", "patrol_type"],
            **(params_dict.get("pe_cols_to_string") or {}),
        )
        .call()
    )

    filter_patrol_events = (
        apply_reloc_coord_filter.validate()
        .set_task_instance_id("filter_patrol_events")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            df=fetch_patrol_events,
            roi_gdf=None,
            roi_name=None,
            **(params_dict.get("filter_patrol_events") or {}),
        )
        .call()
    )

    split_patrol_traj_groups = (
        split_groups.validate()
        .set_task_instance_id("split_patrol_traj_groups")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            df=traj_colormap,
            groupers=groupers,
            **(params_dict.get("split_patrol_traj_groups") or {}),
        )
        .call()
    )

    split_pe_groups = (
        split_groups.validate()
        .set_task_instance_id("split_pe_groups")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            df=pe_colormap,
            groupers=groupers,
            **(params_dict.get("split_pe_groups") or {}),
        )
        .call()
    )

    persist_traj_gpkg = (
        persist_df.validate()
        .set_task_instance_id("persist_traj_gpkg")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            df=patrol_traj,
            filename="patrol_trajectories",
            filetype="gpkg",
            root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            **(params_dict.get("persist_traj_gpkg") or {}),
        )
        .call()
    )

    persist_traj_parquet = (
        persist_df.validate()
        .set_task_instance_id("persist_traj_parquet")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            df=patrol_traj,
            filename="patrol_trajectories",
            filetype="geoparquet",
            root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            **(params_dict.get("persist_traj_parquet") or {}),
        )
        .call()
    )

    persist_events_gpkg = (
        persist_df.validate()
        .set_task_instance_id("persist_events_gpkg")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            df=filter_patrol_events,
            filename="patrol_events",
            filetype="gpkg",
            root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            **(params_dict.get("persist_events_gpkg") or {}),
        )
        .call()
    )

    persist_events_parquet = (
        persist_df.validate()
        .set_task_instance_id("persist_events_parquet")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            df=filter_patrol_events,
            filename="patrol_events",
            filetype="geoparquet",
            root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            **(params_dict.get("persist_events_parquet") or {}),
        )
        .call()
    )

    base_map_defs = (
        set_base_maps.validate()
        .set_task_instance_id("base_map_defs")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(**(params_dict.get("base_map_defs") or {}))
        .call()
    )

    patrol_events_map_layers = (
        create_point_layer.validate()
        .set_task_instance_id("patrol_events_map_layers")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            layer_style={"fill_color_column": "event_type_colormap"},
            legend=None,
            tooltip_columns=["patrol_serial_number", "event_type", "time"],
            **(params_dict.get("patrol_events_map_layers") or {}),
        )
        .mapvalues(argnames=["geodataframe"], argvalues=split_pe_groups)
    )

    patrol_traj_map_layers = (
        create_polyline_layer.validate()
        .set_task_instance_id("patrol_traj_map_layers")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            layer_style={
                "get_width": 3,
                "width_units": "pixels",
                "color_column": "patrol_traj_colormap",
            },
            tooltip_columns=["patrol_serial_number", "patrol_type"],
            **(params_dict.get("patrol_traj_map_layers") or {}),
        )
        .mapvalues(argnames=["geodataframe"], argvalues=split_patrol_traj_groups)
    )

    combined_traj_and_pe_map_layers = (
        groupbykey.validate()
        .set_task_instance_id("combined_traj_and_pe_map_layers")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                all_keyed_iterables_are_skips,
            ],
            unpack_depth=1,
        )
        .partial(
            iterables=[patrol_traj_map_layers, patrol_events_map_layers],
            **(params_dict.get("combined_traj_and_pe_map_layers") or {}),
        )
        .call()
    )

    traj_patrol_events_ecomap = (
        draw_ecomap.validate()
        .set_task_instance_id("traj_patrol_events_ecomap")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            tile_layers=base_map_defs,
            static=False,
            max_zoom=20,
            **(params_dict.get("traj_patrol_events_ecomap") or {}),
        )
        .mapvalues(argnames=["geo_layers"], argvalues=combined_traj_and_pe_map_layers)
    )

    traj_pe_ecomap_html_urls = (
        persist_text.validate()
        .set_task_instance_id("traj_pe_ecomap_html_urls")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            root_path=os.environ["ECOSCOPE_WORKFLOWS_RESULTS"],
            **(params_dict.get("traj_pe_ecomap_html_urls") or {}),
        )
        .mapvalues(argnames=["text"], argvalues=traj_patrol_events_ecomap)
    )

    traj_pe_map_widgets_single_views = (
        create_map_widget_single_view.validate()
        .set_task_instance_id("traj_pe_map_widgets_single_views")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            title="Combined Patrol Events and Trajectories",
            **(params_dict.get("traj_pe_map_widgets_single_views") or {}),
        )
        .map(argnames=["view", "data"], argvalues=traj_pe_ecomap_html_urls)
    )

    traj_pe_grouped_map_widget = (
        merge_widget_views.validate()
        .set_task_instance_id("traj_pe_grouped_map_widget")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            widgets=traj_pe_map_widgets_single_views,
            **(params_dict.get("traj_pe_grouped_map_widget") or {}),
        )
        .call()
    )

    patrol_dashboard = (
        gather_dashboard.validate()
        .set_task_instance_id("patrol_dashboard")
        .handle_errors()
        .with_tracing()
        .skipif(
            conditions=[
                any_is_empty_df,
                any_dependency_skipped,
            ],
            unpack_depth=1,
        )
        .partial(
            details=workflow_details,
            widgets=[traj_pe_grouped_map_widget],
            groupers=groupers,
            time_range=time_range,
            **(params_dict.get("patrol_dashboard") or {}),
        )
        .call()
    )

    return patrol_dashboard
